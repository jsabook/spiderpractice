{"./":{"url":"./","title":"Introduction","keywords":"","body":"Introduction Copyright © dsx2016.com 2019 all right reserved，powered by Gitbook该文章修订时间： 2020-11-18 14:45:44 "},"python的excel操作.html":{"url":"python的excel操作.html","title":"python的excel操作","keywords":"","body":"初始操作 引用三个库 import xlrd import xlwt from xlutils.copy import copy 通过xlrd与xlet与xlutilds.copy的导入来对excel做基本的操作。 建立excel表格 内容：建立excel 首先现在python里面建立一个excel表格，最后当完成对这个表的操作时，保存在文件夹里即可。 建立： workbook=xlwt.Workbook(encoding=””) worksheet=workbook.add____sheet(sheet_name) 保存： workbook.save(保存路径) 对excel进行操作 打开excel表格，读取数据 对excel进行内容填充 对excel进行内容修改 打开excel表格，读取数据 获取excel Date=xlrd.open__workbook(存储路径) 获取工作表，三种方法 table = data.sheets()[index_number] index___numble 表示第几张工作表，这个方式也可以用来迭代 通过索引的方式 table = data.sheet_by_index(index_number) 这个是通过表的名字来获取 table = data.sheet_by_name(sheet_name) 获取单元格的相关信息 数目：table.nrows ,获取行数 table.ncols ,获取列数 获取单元格的内容： 整行整列获取 table.row_values(row_index) table.col_values(col_index) 一个一个获取： table.cell(row_index, col_index).value 填充单元格内容： worksheet.write(r, c, label='', style=) 对excel进行内容修改 其实就是用copy方法将表复制，进行相关的操作，最后保存回去就可以了 copyBook = copy(readbook) #拷贝整个excel copyBook = copy(readSheet) #拷贝第一张表，然后拿到要修改的表 writeSheet = copybook.get_sheet(0) writeSheet.write(1,0,'asd')#修改表一种的第二行第一个单元格内容 copybook.save('c:\\\\t2.xls') Copyright © dsx2016.com 2019 all right reserved，powered by Gitbook该文章修订时间： 2020-11-15 11:51:42 "},"format函数.html":{"url":"format函数.html","title":"format函数","keywords":"","body":"Format中通过{}和” : “ 来替代%。 Format前后的映射关系 通过位置 通过关键字 通过下标 格式限定符： 放在{}里面的 当中英文同时出现导致对不齐怎么办，用空格· 如： \"{0:^10}\\t{1:{3}^10}\\t{2:^10}\".format(u[0],u[1],u[2],chr(12288)) 使用format模板： tplt = \"{0:^10}\\t{1:{3}^10}\\t{2:^10}\" print(tplt.format(\"排名\",\"学校名称\",\"总分\",chr(12288))) Copyright © dsx2016.com 2019 all right reserved，powered by Gitbook该文章修订时间： 2020-11-15 11:44:38 "},"mysql简单操作.html":{"url":"mysql简单操作.html","title":"mysql简单操作","keywords":"","body":"常见操作 连接： 建立库 或者 create database basename 删除数据库： 或者 drop database basename 删除数据库： 或者drop database basename 选择数据库 USE DATABSE DATABASEBANE(use database basename) 建立表 栗子： 删除数据表： 在表中加入数据 在表中提取数据 删除数据: 修改数据： 删除数据： where 是用来限制条件的。 like是模糊比较 union是将多张表一起联合输出 • 参数含义： expression1, expression2, ... expression_n: 要检索的列。 • tables: 要检索的数据表。 • WHERE conditions: 可选， 检索条件。 • DISTINCT: 可选，删除结果集中重复的数据。默认情况下 UNION 操作符已经删除了重复数据，所以 DISTINCT 修饰符对结果没啥影响。 • ALL: 可选，返回所有结果集，包含重复数据。 Python对mysql的操作 连接 做两种操作（是否会对源数据产生影响） 连接 两种操作： 不会对源数据产生影响就不用提交事务 会对源数据产生影响就必须提交事务 不管怎么样最后都要关闭连接 Pymysql也能操作从这个可以看出来 规范的提交一次数据代码： 使用ORM 是不是用pymysql特别麻烦，现在有一个特别简单的工具ORM,即sqlalchemy 下面来介绍sqlalchemy 连接sqlalchemy http://docs.sqlalchemy.org/en/latest/core/engines.html#sqlalchemy.create_engine 连接 比如如这个 'mysql+pymysql://{}:{}@{}:{}/{}?charset=utf8'.format(USERNAME,PASSWORD, HOSTNAME,PORT,DATABASE) 数据表 映射肯定是需要数据表的不然python里面的怎么实现和数据库中的一一对应。 然后是怎么映射到数据库，需要执行这一句语句 其中的base是怎么来的 from sqlalchemy.ext.declarative import declarative_base 执行完上面的语句后就是映射python类到数据表中了 然后需要对数据表进行操作，比如插入，查找，更新，删掉。首先这些操作都是在session中进行的。session这里解释一下就是理解成数据库与python构建成的一条通道，任何对数据表的操作都要经过这里。 如何构建一个session 两种方法 from sqlalchemy.orm import sessionmaker Session=sessionmarker(bind=engin) 或者 Session=sessionmarker() Session.configure(bind=engine) 但是最后都要 session=Session() 插入 插入肯定插入的是一个数据表字段，那怎么生成一个数据表字段 ed_user = User(name='ed', fullname='Ed Jones', password='edspassword') 这样ed_user就是一个字段 然后提交 session.add(ed_user) session.commit() 那么如何一次增加多个字段 session.add_all([ ... User(name='wendy', fullname='Wendy Williams', password='foobar'), ... User(name='mary', fullname='Mary Contrary', password='xxg527'), ... User(name='fred', fullname='Fred Flinstone', password='blah')]) 用session.add)all([,])将字段变成列表的形式。 脏（dirty）数据与新（new）数据:二者得到区别在于是否对session进行了commit() 如果没有进行commit。那么数据表中的信息就没有发生改变。session.dirty 如果进行了commit。那么数据表中的信息就是已经进行了更新。session.new 如下面 >>>session.dirty IdentitySet([]) >>>session.new IdentitySet([]) 回滚 session.rollback() 查询 这里要涉及到 query（）,查找是通过session的query方法。这里有两种查查找，session.query(User,User.id).all()和session.query(user).order_by(User.id).all()。这两种的sql语句如下所示 比如session.query(User).order_by(User.id)等价于sql语句 SELECT users.id AS users_id, users.name AS users_name, users.fullname AS users_fullname, users.password AS users_password FROM users ORDER BY users.id 这个的意思是按照user.id进行排序后输出。 session.query(User,User.id) SELECT users.name AS name_label FROM users 可以看出前者是按照id字段排序获取到的是整个字段。后者是获取到一部分字段。比如这里获取到的是user.id. 切片查询。这个query也是能够进行切片查询，如下所示： >>> >>> for u in session.query(User).order_by(User.id)[1:3]: ... print(u) 过滤查找。 这个才是核心。因为我们查找数据库，肯定是获取数据表中的某几条数据这里是采用query的filter().可以多次利用filter进行多次限制，如下所示： session.query(User).filter(User.name=='ed').filter(User.fullname=='Ed Jones'): 过于filter()这里有三种过滤方式： 等于过滤，获取等于某些值得记录 query.filter(User.name == 'ed') 不符合过滤，当不符合某些要求是才会获取 query.filter(User.name != 'ed') 模糊查找，这个是mysql里面比较有意思的 不忽视大小写 query.filter(User.name.like('%ed%')) 忽视大小写 query.filter(User.name.ilike('%ed%')) 列表匹配 in query.filter(User.name.in_(['ed', 'wendy', 'jack'])) works with query objects too: query.filter(User.name.in_( session.query(User.name).filter(User.name.like('%ed%')) )) 自然有in 了，就会有not in。这里的not in 是在filter()条件里面加入~ query.filter(~User.name.in_(['ed', 'wendy', 'jack'])) 逻辑符号AND，OR from sqlalchemy import and_ query.filter(and_(User.name == 'ed', User.fullname == 'Ed Jones')) from sqlalchemy import or_ query.filter(or_(User.name == 'ed', User.name == 'wendy')) 就是session.query(表).filter(条件).order_by(排序) 获取查询到的列表知乎，但是你只想要一个，或者想要全部怎么搞？在最后加上all(),first() 或者one() 删除 delete >>> a = session.query(Article).get(10) >>> session.delete(a) >>> session.commit() 更新 update(查询后再提交) >>> >>> a = session.query(Article).get(10) >>> a.title = 'My test blog post' >>> session.add(a) >>> session.commit() 以上是一张数据表，但是如果有两张数据表想关联起来怎么搞？ 外键 一对多外键 这里只需要两步，在创立数据表时候，先在两张表中创建关系，再创建外键就可以了。可以理解成一对多关系肯定是先创建关系了，才能添加外键 如何创建一张表与其它表的关系 name=relation（“Tablename”） 如何创建外键 keyname=Column(Type,Foreignkey(“tablename.keyname”) class User(Base): __tablename__ = 'users' id = Column(Integer, primary_key=True) username = Column(String(64), nullable=False, index=True) password = Column(String(64), nullable=False) email = Column(String(64), nullable=False, index=True) B articles = relationship('Article') class Article(Base): __tablename__ = 'articles' id = Column(Integer, primary_key=True) title = Column(String(255), nullable=False, index=True) content = Column(Text) user_id = Column(Integer, ForeignKey('users.id')) A user= relationship('User') User可以通过articles来访问到Article.Article是通过user_id来一一对应。Article通过user来访问User 简便的写法 去掉A处代码，在B处代码贴上backref 如下 class User(Base): __tablename__ = 'users' id = Column(Integer, primary_key=True) username = Column(String(64), nullable=False, index=True) password = Column(String(64), nullable=False) email = Column(String(64), nullable=False, index=True) articles = relationship('Article'，backref=‘user’) class Address(Base): ... __tablename__ = 'addresses' ... id = Column(Integer, primary_key=True) ... email_address = Column(String, nullable=False) ... user_id = Column(Integer, ForeignKey('users.id')) ... ... user = relationship(\"User\", back_populates=\"addresses\") 如何在通过外键添加数据 >>> jack = User(name='jack', fullname='Jack Bean', password='gjffdd') >>> jack.addresses >>> jack.addresses = [ ... Address(email_address='jack@google.com'), ... Address(email_address='j25@yahoo.com')] >>> session.add(jack) >>> session.commit() 外键的获取 >>> jack = session.query(User).\\ ... filter_by(name='jack').one() >>> jack.addresses 外键的查找 session.query(User, Address).\\ ... filter(User.id==Address.user_id).\\ ... filter(Address.email_address=='jack@google.com').all(): 这样使用起来是不是特别玛法，我们可以使用join来。 >>> session.query(User).join(Address).\\ ... filter(Address.email_address=='jack@google.com').\\ ... all() Copyright © dsx2016.com 2019 all right reserved，powered by Gitbook该文章修订时间： 2020-11-18 13:49:28 "},"python与cmd.html":{"url":"python与cmd.html","title":"python与cmd","keywords":"","body":"如何在命令行下运行python文件？ 先到达改python文件下 在再文件命令行后加上python （文件名）.py就行了 栗子： Python输出语句到cmd中 python执行系统命令后获取返回值的几种方式 执行系统命令，没有返回值 执行系统命令，按行输出返回值 如何通过命令行方式将python程序封装成exe程序 1、 先到达python所在的文件夹中 通过pyinstaller程序和其属性： Pyinstaller有哪些常见的属性？ 官方翻译文章 python与命令行参数 首先如果要用到命令行参数就要导入sys这个库 即import sys Copyright © dsx2016.com 2019 all right reserved，powered by Gitbook该文章修订时间： 2020-11-15 12:00:09 "},"python对图像的处理.html":{"url":"python对图像的处理.html","title":"python对图像的处理","keywords":"","body":"主要用到的就是PIL库 如何获取到一张图片上的某一个点的RGB值？ from PIL import Image image=Image.open('ca.jpg') piexl=image.load()[1,20]#图片上某一点的x,y坐标的RGB值，获取到的是一个三元数组 print(piexl) Copyright © dsx2016.com 2019 all right reserved，powered by Gitbook该文章修订时间： 2020-11-15 11:52:09 "},"redis.html":{"url":"redis.html","title":"Redis","keywords":"","body":"Centos如何安装redis 分为几步： Redis连接服务器/本地。有两个要求，第一个相应的端口是打开的，第二个能ping通 然后用python就可以通了 数据类型： Redis支持五种数据类型：string（字符串），hash（哈希），list（列表），set（集合）及zset(sorted set：有序集合)。 Copyright © dsx2016.com 2019 all right reserved，powered by Gitbook该文章修订时间： 2020-11-15 12:05:45 "},"Re正则.html":{"url":"Re正则.html","title":"Re正则","keywords":"","body":"Re查找 基于正则表达式的查找 正则表达式就是将r.textl里面的部分内容，或全部内容进行查找，摘出相应的字符串，所以正则表达式查找，输入和输出都是字符串 正则表达式的语法 正则表达式分为两种一种是r”(原生字符表达式)，还有一种string类型 正则表达式的使用方式 Match对象 Match是一次匹配的结果里面包含很多信息（和requestsli里面返回的Sponse对象一样，包含了很多信息） Re库的主要功能函数 Re.search(pattern,string,flags=0) 返回match对象 Pattern是正则表达式的字符串或者是·原生字符。 String是待匹配的字符串 Flags是正则表达式使用时候的控制标记 re库的一种等价用法 最小匹配 返回的有match类型和string类型和列表类型 Re正则 \\w \\W \\s \\S \\G \\d \\D \\a \\n \\t ^ $ . […] … * + ? { } { } a\\ b () 上面是常见的正则匹配规则，然后python对正则有个模块，现在来介绍这个模块的方法与属性 import re 其中pattern 是匹配规则，string是待匹配字符串，flag是匹配信号 math()。 re.match(pattern, string, flags=0)，获取到的是一个匹配对象Match对象。其中Match有哪些属性？Match.group(1)获取到匹配的一个结果。Match.groupdict()以字典的形式返回匹配的第一个字符与最后一个字符。Match.start()以返货匹配的第一个字符的下标，和最后一个字符的下标 search()。 与match（）基本一样唯一不同的是match不是匹配开头。 finditer() finditer(*pattern*, *string*, *flags=0*) 与findall()差不多，但获取到的是Matchs findall() findall(*pattern*, *string*, *flags=0*) ​```获取到的是可匹配的字符列表 ## sub() sub(pattern, repl, string, count=0, flags=0)* ## split() split(pattern, string, maxsplit=0,* flags=0) Copyright © dsx2016.com 2019 all right reserved，powered by Gitbook该文章修订时间： 2020-11-18 13:54:53 "},"SMETPLIB和POP3.html":{"url":"SMETPLIB和POP3.html","title":"SMETPLIB和POP3","keywords":"","body":"用哪两个包？python里面的smtplib和email两个模块，Email用来构造一封邮件，email负责邮件，smtplib用来发送邮件。其中目前最难的是如何编写一封邮件 如何构造一个纯文本？ 这里用MIMETText类构造了一封信。 构造MIMETText里面要添加三个属性值，第一个参数是信的正文内容，第二个参数是信的类型（有三个值text/plain(纯文本) 、html（超文本）） 如何附加信息 如何构造含有邮件主题、邮件发件人、邮件收件人加到电子邮件中？ msgFrom = '15558781880@163.com' #从该邮箱发送 msgTo = '15558781880@163.com' #发送到该邮箱 msg['from'] = msgFrom msg['to'] = msgTo msg['subject'] = 'Python自动邮件-' 最后一个是编码方式？一般是网络编码方式‘utf-8’ 如何发送html超文本邮件？ 方法很简单，在构造MIMEText对象时，把HTML字符串传进去，再把第二个参数由plain变为html就可以了： 四、如何发送正文的时候一起发送附件？ 带附件的邮件可以看做包含若干部分的邮件：文本和各个附件本身，所以，可以构造一个MIMEMultipart对象代表邮件本身，然后往里面加上一个MIMEText作为邮件正文，再继续往里面加上表示附件的MIMEBase对象即可： 五、如何发送图片？ 只要在这个MIMEMultipart总邮箱里面增加 MIMEMultipart的MIMEText从plain改为html，然后在适当的位置引用图片 如何使得一份邮件同时有plain和html？ 用email构造邮件的方式已经完成，现在如何把这份邮件发出去？ 发送 Smtplib实例相当于一个手机发送邮件的客户端 class smtplib.SMTP_SSL(host='', port=0, local_hostname=None, keyfile=None, certfile=None, [timeout, ]context=None, source_address=None) 参数解释：一个SMTP_SSL实例的行为与实例的行为完全相同 SMTP。SMTP_SSL应该用于需要从连接开始使用SSL并且使用starttls()不合适的情况。如果主机没有指定，则使用本地主机。如果 端口为零，则使用标准的SMTP-over-SSL端口（465）。可选参数local_hostname，timeout和source_address与它们在SMTP类中的含义相同。 上下文也是可选的，可以包含SSLContext和允许配置安全连接的各个方面。请阅读最佳实践的安全注意事项。 一些异常 其余异常查看文档：python3.6 SMTP对象 常见方法： connect SMTP.connect（host ='localhost'，port = 0 ） 连接到给定端口上的主机。默认值是连接到标准SMTP端口（25）的本地主机。如果主机名以冒号（':'）后跟数字结尾，则该后缀将被剥离，并将该数字解释为要使用的端口号。如果在实例化过程中指定了主机，则此方法由构造函数自动调用。返回服务器在连接响应中发送的响应代码和消息的2元组。 Login: SMTP.login（user，password，*，initial_response_ok = True ） 登录需要验证的SMTP服务器。参数是用户名和密码进行身份验证。如果之前没有 EHLO或HELO命令此会话，则此方法EHLO 首先尝试ESMTP 。如果身份验证成功，此方法将正常返回 Send SMTP.sendmail(from_addr，to_addrs，msg，mail_options = []，rcpt_options = []) （注意：to_addrs是一个列表，可以用来一次发给多个人，一个字符串会被当成列表中只有一个元素） Quit: SMIT.quit() 用来结束smit的连接和这个实例。 栗子： Poplib(poplib也是创建一个收信的手机客户端) 我们通过获取+解析邮件来获取邮件内容 首先是连接 接着是登入账号，你的实例就变成了一个客户端 Poplib.POP3实例的常见 Python pop 解析邮件 Copyright © dsx2016.com 2019 all right reserved，powered by Gitbook该文章修订时间： 2020-11-18 14:45:04 "},"装饰器.html":{"url":"装饰器.html","title":"装饰器","keywords":"","body":"装饰器 装饰器也是函数，是可以装饰函数的函数: 对没有参数的函数进行装饰 def decorater(func): def wrapper()： ----ship----在func函数前进行修改 func() ----ship----在func函数后进行修改 return wrapper 这里解释一下为什么一定要在函数里面再自定义一个函数，很简单。装饰器肯定要把装饰后的函数返回回来，但是如果只有一个函数，那么怎么可能返回自己。因此需要再函数里面再定义一个函数，才能返回装饰后的函数。 使用方式 func=decorater(func) 这样子太丑了，因此只要在定义的函数前面加上@decorater就可以了。 @decorater def func() pass 如果要装饰的函数带参数怎么办？？ 也很简答只要，写装饰器的时候，带上万能参数就可以了。 带参数的函数的装饰器。 def decorater(func): def wrapper(*arg,**args)： ----ship----在func函数前进行修改 func(*arg,**args) ----ship----在func函数后进行修改 return wrapper 那如果是装饰器带参数怎么办，也很简单。只要把装饰器外面在加上一层就可以了。因此需要返回两次才能将装饰后的函数返回。 def function(args): def decorater(func): def wrapper(*arg,**args)： ----ship----在func函数前进行修改 func(*arg,**args) ----ship----在func函数后进行修改 return wrapper return decorater 这篇文章还是蛮不错的 http://www.wklken.me/posts/2013/07/19/python-translate-decorator.html Copyright © dsx2016.com 2019 all right reserved，powered by Gitbook该文章修订时间： 2020-11-18 14:44:22 "},"python与cookies.html":{"url":"python与cookies.html","title":"python与cookies","keywords":"","body":"我知道的有三个地方与cookies有关 1、 selenium 2、 requests(高级的session()方法) 3、 scrapy 我们如何得到cookies? 一、 Selenium，Selenium操作cookies有4个方法 get_cookies(): 获得所有cookie信息。 get_cookie（name）：返回字典的key为name的cookie add_cookie(cookie_dict):添加cookie。“cookie_dict”指字典对象，必须有name和value值。 delete_cookie(name,optionsString):删除cookie信息。“name”是要删除的cookie的名称。“optionsString”是该cookie的选项，目前支持的选项包括“路径”，“域” delete_all_cookies():删除所有cookie信息。 那我们如何获取登录所需要的cookies信息？ 实现代码 二、 requests(高级的session()方法) requests可以看成一次对网页的无痕浏览，session可以看成一次普通状态下的浏览器。 Requests对cookies的操作session都能做，而session对cookies的操作requests却不能完成，虽说session是对requests的继承。因此我们这里只讲requests对session的操作。 如何获得cookies？ 第一种方法是把cookies变成 LWPCookieJar类型,cookies进行加载和存储。 首先转换类型 这样_session就会增加save()和load()两个方法 如何通过session获得cookies? 代码： 如何把LWPCookieJar类型的cookies加到session里面,用login()方法： 追加cookies: 这里面加载的cookies是LWPCookieJar类型。 第二种方法获得其他类型的cookie数据： 列表类型： 字典类型： LWPCookieJar类型与字典类型的相互转换： 三、 scrapy和cookies 要明白下面两者的区别 meta={'cookiejar':1}表示这个请求的cookiejar是几，下次要调用这个cookiejar时候，只要写上这个value就能调用相对应的cookiejar meta={'cookiejar':response.meta['cookiejar']}表示使用上一次response的cookie，写在FormRequest.from_response()里post授权 meta={'cookiejar':True}表示使用授权后的cookie访问需要登录查看的页面 cookies除了可以用在登录时候，有时候cookie里面的东西也许会被js拿过来添加到postdata中。 Copyright © dsx2016.com 2019 all right reserved，powered by Gitbook该文章修订时间： 2020-11-15 12:04:52 "},"反反爬.html":{"url":"反反爬.html","title":"反反爬","keywords":"","body":"有些网站必须在headers里提供refer才能获取到信息： 比如知乎live、mm131网站 知乎live的头信息必须提供 Headers={ “Host”:”api.zhihu.com”, “Origin”:”https://www.zhihu.com”, “Referer”:”https://www.zhihu.com/lives” } Mm131的头信息必须添加上 Headers={“Referer”:” 'http://www.mm131.com/' + keyName + \"/\"”} Copyright © dsx2016.com 2019 all right reserved，powered by Gitbook该文章修订时间： 2020-11-18 14:03:15 "},"python的Selenium_Python的使用方法的.html":{"url":"python的Selenium_Python的使用方法的.html","title":"python的Selenium_Python的使用方法的","keywords":"","body":"如何使用seleium？ 要先写以下代码： from selenium import webdriver browser = webdriver.Firefox() 实例化一个火狐浏览器 连接url browser.get(\"http://baidu.com/\") 关闭浏览器 browser.close() http://www.cnblogs.com/taceywong/p/6602715.html 定位元素： 这里介绍两种定位方式 定位一个元素 tag定位 browser.find_element_by_xx(tag_name)(\"xx\"(tag_value)) xpath定位 browser.find_element_by_xpath(\"xx\"(xpath表达式)) 查找多个节点 只要在element后面加上s就可以了。 如：browser.find_elements_by_xx(tag_name)(\"xx\"(tag_value)) 与浏览器的交互: 定位元素后，比较常见的方法有send_keys()这个可以在文本框中输入内容，也可以输入回车。clear()是清空文字。点击按钮是用click() 如这个例子 input=browser.find_element_by_id(“p”) input.click() 获取属性 tag=browser.find_element_by_id(“p”) print(tag.get_attribute()) 获取文本值 tag=browser.find_element_by_id(“p”) print(tag.text) 获取一个节点在web页面中的外观信息 比如location,size,其中location是获取节点在页面中的相对位置。size是获取这个显示出来图案的大小。 获取页面的源码 pageSource = browser.page_source 获取页面的截图 browser.save_screenshot(\"screenshut.png\") 等待，selenium等待有两种。分别是显示等待和隐式等待。这两种都不好。这里介绍一下两者折中的方法。半显示半隐式等待 from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC driver = webdriver.Firefox() driver.get(\"http://somedomain/url_that_delays_loading\") wait= WebDriverWait(driver,10) element = wait.until( EC.presence_of_element_located((By.ID, \"myDynamicElement\")) ) 上面就是实例化waite ，在加入until的条件 cookies处理 获取cookies browser.get_cookies() 加入cookies browser.add_cookie({}) 删除cookies browser.delete_all_cookies() Copyright © dsx2016.com 2019 all right reserved，powered by Gitbook该文章修订时间： 2020-11-15 11:47:13 "},"Python爬虫提升速度.html":{"url":"Python爬虫提升速度.html","title":"Python爬虫提升速度","keywords":"","body":"这里主要讲两个方法来提升爬虫速度以及如何优化 通过函数来产生方法 通过_thread.start_new_thread()来产生新的线程，括号里面要加的是函数名和一些参数 这个是比较低级的，自主添加性不高，不过可以用来临时做要求性不高的东西 通过模块来提高速度 通过threading模块来产生新的线程 模块里的常见方法 run():用来产生表示线程的方法 start():用来启动一个线程 join():只有当线程加载完时，才会接着主线继续执行下去 isAlive():如果线程还在运行就会返回一个True否则就会返回一个False getName():获取一个线程的名字 setName()设置一个线程的名字 格式： 注：只要线程开始运行就会自动运行run方法，所以我们只要对run()方法进行重写就可以了最重要的就是run方法，如果说_thread（）函数与threading模块有什么不同，那就是threading里面得run方法能放很多函数，但是_thread()只能有一个函数 通过multiprocessing模块来产生新的进程 Multiprocessing模块的方法与threading的模块方法类似 爬虫得优化：使用queue队列型，这样就能做到自动结束，而不用向列表一样特别烦，queue一般使用：(queue模块的使用) 1、创建： import Queue myqueue = Queue.Queue(maxsize = 10) 2、将值放到队列里面： myqueue.put(10)， put()有两个参数，第一个item为必需的，为插入项目的值；第二个block为可选参数，默认为1。如果队列当前为空且block为1，put()方法就使调用线程暂停,直到空出一个数据单元。如果block为0，put方法将引发Full异常。 3、将一个值从列队里面拿出来 myqueue.get()调用队列对象的get()方法从队头删除并返回一个项目。可选参数为block，默认为True。如果队列为空且block为True，get()就使调用线程暂停，直至有项目可用。如果队列为空且block为False，队列将引发Empty异常。 今天爬虫事情暂停我还没写的爬虫速度优化有 使用pool+queue的多进程爬虫 多协程爬虫 还有的优化爬虫速度的方法： 这些建议的url 为何大量网站不能抓取?***爬虫突破封禁的6**种常见方法*** 最简便的爬虫效率提升方法\\ def insert_info(): ''' 通过遍历游戏分类页面获取所有直播间 ''' session = requests.session() pagecontent = session.get(Directory_url).text pagesoup = BeautifulSoup(pagecontent) games = pagesoup.select('a') col.drop() for game in games: links = game[\"href\"] Qurystr = \"/?page=1&isAjax=1\" gameurl = HOST + links + Qurystr gamedata = session.get(gameurl).text flag = get_roominfo(gamedata) aggregateData() from multiprocessing.dummy import Pool pool = Pool() def insert_info(): ''' 通过遍历游戏分类页面获取所有直播间 ''' session = requests.session() pagecontent = session.get(Directory_url).text pagesoup = BeautifulSoup(pagecontent) games = pagesoup.select('a') gameurl = [HOST + url[\"href\"] + \"/?page=1&isAjax=1\" for url in games] col.drop() g = lambda link: session.get(link).text gamedata = pool.map(g, gameurl) ginfo = lambda data: get_roominfo(data) pool.map(ginfo, gamedata) aggregateData() aggregateData() 同样通过Directory_url这一地址获取页面中所有标签，然后通过pool.map(g, gameurl)完成'href'值的获取，最后再用一次pool.map(ginfo,gamedata)完成所有页面内容的提取和入库。再一次测试，只需要33.1s即可完成。 所以说，如果你的爬虫也要处理类似的过程，不妨尝试一下multiprocessing。 更多详细信息可以参考官方文档。 Copyright © dsx2016.com 2019 all right reserved，powered by Gitbook该文章修订时间： 2020-11-15 11:57:26 "},"python异步请求.html":{"url":"python异步请求.html","title":"python异步请求","keywords":"","body":"async with aiohttp.ClientSession() as session: async with session.get('http://httpbin.org/get') as resp: await resp.text() 这是一个最简单的用法，那么如何添加headers或者修改代理呢？ 添加头部 headers={\"Authorization\": \"Basic bG9naW46cGFzcw==\"} async with aiohttp.ClientSession(headers=headers) as session: async with session.get(\"http://httpbin.org/headers\") as r: json_body = await r.json() 添加cookies url = 'http://httpbin.org/cookies' cookies = {'cookies_are': 'working'} async with ClientSession(cookies=cookies) as session: async with session.get(url) as resp: assert await resp.json() == { \"cookies\": {\"cookies_are\": \"working\"}} 如何分享cookies async with aiohttp.ClientSession() as session: await session.get( 'http://httpbin.org/cookies/set?my_cookie=my_value') filtered = session.cookie_jar.filter_cookies( 'http://httpbin.org') assert filtered['my_cookie'].value == 'my_value' async with session.get('http://httpbin.org/cookies') as r: json_body = await r.json() assert json_body['cookies']['my_cookie'] == 'my_value' Copyright © dsx2016.com 2019 all right reserved，powered by Gitbook该文章修订时间： 2020-11-15 11:58:25 "},"进程与线程.html":{"url":"进程与线程.html","title":"进程与线程","keywords":"","body":"我们要明白两队关系：异步与同步，并发与并行 我们把进程内的这些“子任务”称为线程（Thread）。 多进程： Unix/Linux下，可以使用fork()调用实现多进程。 要实现跨平台的多进程，可以使用multiprocessing模块。 进程间通信是通过Queue、Pipes等实现的。 Multiprocessing 守护进程： 默认情况下，创建的子进程非守护进程，既主进程如果执行完毕的，而子进程没有执行完毕，主进程会等待子进程执行完毕后再退出。 及设置deame=True p.daemon = True p.start() 相关方法： 多进程举例： 更换ip 我们可以用线程来写一个获取可用IP的的线程分支 注意 Python 多进程默认不能共享全局变量，因此我们只能用停止主线程来运行其他线程 #如果一定要运行多进程的话可以通过 #（multiprocessing.Value(\"d\",10.0)，数值） # （multiprocessing.Array(\"i\",[1,2,3,4,5])，数组） # （multiprocessing.Manager().dict()，字典） # （multiprocessing.Manager().list(range(5))）。 # 进程通信（进程之间传递数据）用进程队列（multiprocessing.Queue()，单向通信）， # 管道（ multiprocessing.Pipe() ，双向通信）。 但是这个管道不适合查找IP 原因： Copyright © dsx2016.com 2019 all right reserved，powered by Gitbook该文章修订时间： 2020-11-18 14:07:29 "},"crawlspider.html":{"url":"crawlspider.html","title":"Crawlspider","keywords":"","body":"crawlspider 是继承与spider。和spider一样。但是多了一个属性。rule。其中rule是一个Rule为元素的集合。那Rule又是什么？下面是给出的官方定义 class scrapy.spiders.Rule(link_extractor, callback=None, cb_kwargs=None, follow=None, process_links=None, process_request=None) 其实这里主要就是两个参数比较重要，link_extractor和callback可以这样理解一个Rule就是两个参数。第一个参数，抓取什么URL去生成Request，第二个是生成的Request获取到的相应返回哪里。主要就是这两个参数。那link_extractor又是怎么生成的呢？ Copyright © dsx2016.com 2019 all right reserved，powered by Gitbook该文章修订时间： 2020-11-15 11:42:17 "},"base编码与md5.html":{"url":"base编码与md5.html","title":"base编码与md5","keywords":"","body":"base64、base32、base16编码解码 base64加密解密 加密： >>> import base64 >>> encode = base64.b64encode(b'I love you') >>> encode b'SSBsb3ZlIHlvdQ==' 解密： >>> import base64 >>> decode = base64.b64decode(b'SSBsb3ZlIHlvdQ==') >>> decode b'I love you' 二、base32加密解密 跟base64相似就是将base64.b64encode变成base64.b32encode 加密： >>> import base64 >>> encode = base64.b32encode(b'I love you') >>> encode b'JEQGY33WMUQHS33V' 解密： >>> import base64 >>> decode = base64.b32decode(b'JEQGY33WMUQHS33V') >>> decode b'I love you' base16加密解密 类似的将base64.b32encode变成base64.b16encode 加密解密过程与base64、base32一样，这里就不在赘述 md5 在python3的标准库中，已经移除了md5，而关于hash加密算法都放在hashlib这个标准库中，如SHA1、SHA224、SHA256、SHA384、SHA512和MD5算法等。 另：在网上找关于python的md5加密，发现要不是比较旧的不适用当前py版本的文章，或者是说得不够清楚的文章，所以还是自己去看下官方文档比较好，顺便整理下关于md5的使用方法。 对于学习任何一门程序类知识，我都认为去看官方文档这种学习方式最有效的之一，只不过一般这些文档都是英文版的，对于一些学习者来说可能会有一定门槛，但习惯于阅读英文文章，是非常重要的。 建议直接阅读python3的hashlib文档： https://docs.python.org/3/library/hashlib.html?highlight=hashlib#credits 在hashlib库的hash算法中，提供了很多加密算法，有 sha1()、sha224()、sha256()、sha384()、sha512()、blake2b()和 blake2s()、md5()，这些方法都通过统一接口返回一个对象，例如，使用sha256()可以创建一个SHA-256的哈希对象。 当然，进行md5加密算法，就要用到md5()方法： >>> import hashlib >>> m = hashlib.md5() >>> m.update(b'123') >>> m.hexdigest() '202cb962ac59075b964b07152d234b70' # 或者可以这样 >>> hashlib.md5(b'123').hexdigest() '202cb962ac59075b964b07152d234b70' # 也可以使用hash.new()这个一般方法 >>> hashlib.new('md5', b'123').hexdigest() '202cb962ac59075b964b07152d234b70' 以上是对于英文进行md5加密的，如果要对中文进行加密，发现按照上面来写会报错，原因在于字符转码问题，要如下写： >>> import hashlib >>> data = '你好' >>> hashlib.md5(data.encode(encoding='UTF-8')).hexdigest() '7eca689f0d3389d9dea66ae112e5cfd7' 此处先将数据转换成UTF-8格式的，使用网上工具对比下加密的结果，发现有的md5加密工具并不是使用UTF-8格式加密的。 经测试目前发现可以转为UTF-8、GBK、GB2312、GB18030，不分大小写(因为GBK/GB2312/GB18030均是针对汉字的编码，所以md5加密后结果一样)。 除了这些编码格式之外，还会有其他编码的，目前还没发现，等各位补充。 看下面实例： >>> hashlib.md5('你好'.encode(encoding='UTF-8')).hexdigest() '7eca689f0d3389d9dea66ae112e5cfd7' >>> hashlib.md5('你好'.encode(encoding='GBK')).hexdigest() 'b94ae3c6d892b29cf48d9bea819b27b9' >>> hashlib.md5('你好'.encode(encoding='GB2312')).hexdigest() 'b94ae3c6d892b29cf48d9bea819b27b9' >>> hashlib.md5('你好'.encode(encoding='GB18030')).hexdigest() 'b94ae3c6d892b29cf48d9bea819b27b9' 如果你仅仅查md5的写法，看上面实例就够了； 如果你是python新手，想了解这些方法的意思和用法，继续看下面内容。 解析 hashlib.new(name[, data])方法 这是个一般性方法。 name传入的是哈希加密算法的名称，如md5； data传入的是需要加密的数据，可忽略，在之后update()中传入。 >>> m = hashlib.new('md5') >>> m.update(b'123456') >>> m.hexdigest() '202cb962ac59075b964b07152d234b70' 可以使用hashlib.algorithms_guaranteed或者hashlib.algorithms_available这两个内置属性查看hashlib支持哪些加密算法。 hashlib.algorithms_guaranteed是在所有平台上，保证被hashlib模块支持的hash算法名称的集合； hashlib.algorithms_available是在当前运行的python编译器可用的hash算法名称的集合，由于OpenSSL的原因，在这当中可能会出现重复的hash算法名称。 hashlib.algorithms_guaranteed是hashlib.algorithms_available的子集。 看下面输出： >>> hashlib.algorithms_guaranteed {'sha3_384', 'md5', 'blake2s', 'sha3_512', 'blake2b', 'shake_128', 'sha384', 'sha3_256', 'sha1', 'shake_256', 'sha3_224', 'sha512', 'sha256', 'sha224'} >>> hashlib.algorithms_available {'whirlpool', 'ripemd160', 'dsaEncryption', 'sha1', 'SHA224', 'sha512', 'sha256', 'SHA512', 'blake2s', 'blake2b', 'SHA256', 'sha384', 'sha3_256', 'SHA384', 'sha', 'sha224', 'RIPEMD160', 'shake_128', 'sha3_512', 'SHA', 'MD5', 'shake_256', 'DSA', 'sha3_384', 'DSA-SHA', 'ecdsa-with-SHA1', 'md5', 'SHA1', 'dsaWithSHA', 'md4', 'MD4', 'sha3_224'} ` hash.update(arg) 传入arg对象来更新hash的对象。必须注意的是，该方法只接受byte类型，否则会报错。这就是要在参数前添加b 来转换类型的原因： >>> m = hashlib.md5() >>> m.update('123456') TypeError: Unicode-objects must be encoded before hashing 同时要注意，重复调用update(arg)方法，是会将传入的arg参数进行拼接，而不是覆盖。必须注意这一点，因为你在不熟悉update()原理的时候，你很可能就会被它坑了。 也就是说，m.update(a); m.update(b) 等价于m.update(a+b)，看下面例子： >>> m = hashlib.md5() >>> m.update(b'123') >>> m.hexdigest() '202cb962ac59075b964b07152d234b70' >>> m.update(b'456') >>> m.hexdigest() 'e10adc3949ba59abbe56e057f20f883e' >>> hashlib.md5(b'123456').hexdigest() 'e10adc3949ba59abbe56e057f20f883e' hash.hexdigest() 都知道，在英语中hex有十六进制的意思，因此该方法是将hash中的数据转换成数据，其中只包含十六进制的数字。另外还有hash.digest()方法。 Copyright © dsx2016.com 2019 all right reserved，powered by Gitbook该文章修订时间： 2020-11-15 11:32:07 "},"BeautifulSoup库查找与xpath.html":{"url":"BeautifulSoup库查找与xpath.html","title":"BeautifulSoup库查找与xpath","keywords":"","body":"Bs4查找 我们在这里先介绍BeautifulSoup的基本元素以及如何访问这些元素 为什么会有name这个奇葩的获取方式呢,因为假如用了循环遍历以后我们就根本不能判断下一个标签的名字，且不说这个，就算是.parent.parent我们都不能知道这个标签的名字。 r.text的内容经过解析器解析之后才能用BeautifulSoupl的一些处理方法，和一些循环遍历，提取子节点的方法 无过滤查找 基于BS4库的循环遍历方法（通过标签的属性来来实现 一、 下行遍历 将存储到列表后还是能用还是能用标签树的全部标签，因为存到列表里并不影响的type类型。 用法： for child in soup.body.children print(child) for child in soup.body.descendants: print(child): 二、 上行遍历 for parent in soup.a.parents : if parent is None : print(parent) else : print(parent.name) 三、 平行遍历 for sibling in soup.a.next_sibling： print(sibling) for sibling in soup.a.previous_sibling : print(sibling) Bs4的prettify()方法能让html给我们比较好看 注：bs4库将任何html输入都变成utf-8编码 有过滤查找 基于bs4库的信息提取方法 （对xml,json,yaml有不同的提取方法但本质上是相同的，我们在这里先说下xml的信息提取方法） <>.find_all(name,attrs,recursive,string,**kwargs) Name是对标签的名字进行查找，如soup.find_all(‘a’),返回的是一个列表类型，每个元素的最外层是类型。 Attrs的使用方法 直接使用<>,find_all(attrs={}) 间接使用<>.find_all(id=””) ,recursive是否对子孙全部检索recursive=True/Faulse String是<>…<>里面的…。使用方式：soup.find_all(string=re.compile(“B”) 注：我们也可以写简便方式：(……)=.find_all 注：我们这里可以将string类型通过BeautifulSoup进行解析，得到标签树，不过得到的标签树可能是不完整的，但是我们也可以用bs4库的相关操作 .find_all()是结果必须有两个，如果结果只有一个，那么我们就不能直接用find_al()去当列表，后面必须有[0] 我们如何删除其他数据， 1. if isinstance(tr, bs4.element.Tag): 2. tds = tr('td') 3. ulist.append([tds[0].string, tds[1].string, tds[3].strin) 我们可以通过这个方法来删掉不是我们想要的数据 获取文本 获取一个标签内的文本 tag.string 获取多个标签内的文本tag.get_text() bs4与正则的连用 soup.find_all(string=re.compile(“B”)获取到有B的字符数组 我们也可以利用python的异常处理机制 1. try: 2. if html==\"\": 3. continue 4. infoDict = {} 5. soup = BeautifulSoup(html, 'html.parser') 6. stockInfo = soup.find('div',attrs={'class':'stock-bets'}) 7. 8. name = stockInfo.find_all(attrs={'class':'bets-name'})[0] 9. infoDict.update({'股票名称': name.text.split()[0]}) 10. 11. keyList = stockInfo.find_all('dt') 12. valueList = stockInfo.find_all('dd') 13. for i in range(len(keyList)): 14. key = keyList[i].text 15. val = valueList[i].text 16. infoDict[key] = val 17. 18. with open(fpath, 'a', encoding='utf-8') as f: 19. f.write( str(infoDict) + '\\n' ) 20. count = count + 1 21. print(\"\\r当前进度: {:.2f}%\".format(count*100/len(lst)),end=\"\") 22. except: 23. count = count + 1 24. print(\"\\r当前进度: {:.2f}%\".format(count*100/len(lst)),end=\"\") 25. continue 这两个方法都是很不错的方法 返回的都是bs4类型 xpath xpath路径表达式 nodename 可以这样理解nodename是不能单独用的，这个和@是一样的，去规定哪些值。比如response.xpath(“/nodename”)就是获取到第一个节点，而不能这样用。response.xpath(“nodename”)。 “/” 进一层 ”/”可以看成在标签树中更深一层。如下所示 / response=response.xpath(“/html”)，response是获取到深入的一层。 到达了这一层 / 可以将“/“理解成” “//“任意层 可以理解成任意节点，一般是这样用的response.xpath(“.//*)表示获取到该节点下的所有子节点。 “..”表示父亲节点 “@”选取属性 response.xpath(“//p/@key)选区p标签的key属性值 获取文本 获取一个标签内的文本response.xpath(“//p/text()”)获取p标签的文本 标签内的文本response.xpath(“string(.)”).获取response的子节点内的所有文本 谓语表达式[] 是用来过滤的 匹配属性response.xpath(“//*[@class]) 匹配固定的属性值response.xpath(“//*[@key=Value]) 匹配多个属性值 response.xpath(“//*[contains(@class,””)]) 匹配多属性 response.xpath(“//*[contains(@class,””) and contains(@class,””) ]) 运算符 or and mod | …… 按序选择：采用了切片，如response.xpath(“//li[1]/s/text()) Copyright © dsx2016.com 2019 all right reserved，powered by Gitbook该文章修订时间： 2020-11-15 11:41:10 "},"Scrapy/Scrapy1.html":{"url":"Scrapy/Scrapy1.html","title":"Scrapy 1","keywords":"","body":"scrapy scrapy命令 分别来说明： bench 可以用来查看本机的爬取速度 scrapy bench check 检查错误，测试爬虫是否合规.scrapy check spider_name crawl 运行整个爬虫项目 scrapy crawl spider_name 如果不想查看日志的话，就输入 scrapy crawl –-nolog spider_name edit 用来编辑爬虫文件fetch 用来抓取一个url的网址 scrapy fetch url genspider 用来生成爬虫文件scrapy genspider -l 用来查看有哪些爬虫版本可以生成 scrapy genspider -t basic spider_name url 用来生成一个基础的爬虫 setting用来查看setting.py中的配置 scrapy setting –get setting_name list用来查看一个爬虫项目中有几个爬虫文件 scrapy list runspider 用来单独运行一个爬虫文件 scrapy runspider spider_name shell 在命令行中打开交互界面 ，发出一个请求，获取到的是一个response. scrapy shell url view 下载url的源码，并且展示出来， scrapy view url 入门：我们首先要知道的是框架是什么，框架就我认为是已经实现了逻辑关系，我们只要按照要求，在相应的位置填入相应的内容。scrapy框架就是如此。我们先来说下scrapy框架的逻辑 其中最重要的就是engine。 然后然后我们来说需要我们填写的各个部分，其中scrapy start scrapyproject 生成了一个文件夹，其中的文件就是我们需要修改的各个部分。我们先来看下这个文件夹的结构 scrapyproject/ scrapy.cfg project/ __init__.py items.py settings.py pipelines.py middlewares.py spiders/ __init__.py 这个就是文件夹的目录，其中的各个部分就是我们需要填写的。首先我们从spiders文件夹开始说，这个文件夹是专门放爬虫文件。如何生成爬虫文件？scrapy genspider spider 网址 生成的spider如下图所示: 图中出现的是最基本的四个属性与方法。其中start_urls会将列表中的网址依次传给start_requests.其中比较重要的是生成的request请求和返回的response有哪些属性与方法？ request有这些属性与方法，headers,body,cookies,meta这些 其中meta又有哪些值？ 这篇文章还不错 response的属性与方法 那如何获取到网页源码呢？可以这样写response.body.decode(response.encoding) 还有总不可能一个爬虫文件只写一个parse函数把，因此如何跳转到其他函数，这里就要用scrapy.Request中的回调函数callback,当scrapy.Request完成了请求生成了response，就会将response作为回调函数的参数传递给回调函数。那有哪些方式可以传递参数给回调函数？可以通过request的meta来传递参数。 然后是downloadermiddleware spidermiddleware和itme pipeline 用下面这幅图就好理解了 其中1是Pipeline， 2是spider middleware ，3是downloader middleware middlewares.py其中包括downloader middleware和spider middleware。其中downloader middleware怎么写，自己写的downloader middleware只要实现几个方法就行了，如：process_request(request,spider)、process_response(request,response,spidr)和process_sxception(request ,excetion，spider)其中关于这三个方法这篇文章蛮不错的 如：下面就是运用downloader middleware 来换request的headers. 这三个方法我是这样理解的process_request(request,spider)是下载前，就是请求还没有发到Downloader。而process_response(request,response,spidr)则是当response从DownLoader返回response，同是注意的是返回response的同时也会返回生成这个response的request。我猜是这样如果返回的response自己不满意可以返回这个请求接着去再访问。 而spideermiddle用的不太多，以后再考虑。那怎么用middlewave，这个就要涉及到setting关于middlewave这个就是在setting去掉注释： DOWNLOADER_MIDDLEWARES{ } pipeline.py这个文件中放的是对item的处理方式： 一个Pipline有以下四个方法process_item(),open_spider,from_crawl(),close_spider() 格式： class ScrapydownloadertestPipeline(object): def process_item(self, item, spider): return item 而item是一种数据结构，侧重于定义如何在同一个文件中写两个item，就是要保证item中item取名要和pipline相同，即如下图所示： items.py pipline.py 如果如下图所示就不能运行： Copyright © dsx2016.com 2019 all right reserved，powered by Gitbook该文章修订时间： 2020-11-18 14:34:06 "},"Scrapy/Scrapy2.html":{"url":"Scrapy/Scrapy2.html","title":"Scrapy 2","keywords":"","body":"scrapy scrapy命令 分别来说明： bench 可以用来查看本机的爬取速度 scrapy bench check 检查错误，测试爬虫是否合规.scrapy check spider_name crawl 运行整个爬虫项目 scrapy crawl spider_name 如果不想查看日志的话，就输入 scrapy crawl –-nolog spider_name edit 用来编辑爬虫文件 fetch 用来抓取一个url的网址 scrapy fetch url genspider 用来生成爬虫文件 scrapy genspider -l 用来查看有哪些爬虫版本可以生成 scrapy genspider -t basic spider_name url 用来生成一个基础的爬虫 setting用来查看setting.py中的配置 scrapy setting –get setting_name list用来查看一个爬虫项目中有几个爬虫文件 scrapy list runspider 用来单独运行一个爬虫文件 scrapy runspider spider_name shell 在命令行中打开交互界面 ，发出一个请求，获取到的是一个response. scrapy shell url view 下载url的源码，并且展示出来， scrapy view url 入门：我们首先要知道的是框架是什么，框架就我认为是已经实现了逻辑关系，我们只要按照要求，在相应的位置填入相应的内容。scrapy框架就是如此。我们先来说下scrapy框架的逻辑 其中最重要的就是engine。 然后然后我们来说需要我们填写的各个部分，其中scrapy start scrapyproject 生成了一个文件夹，其中的文件就是我们需要修改的各个部分。我们先来看下这个文件夹的结构 scrapyproject/ scrapy.cfg project/ __init__.py items.py settings.py pipelines.py middlewares.py spiders/ __init__.py 这个就是文件夹的目录，其中的各个部分就是我们需要填写的。首先我们从spiders文件夹开始说，这个文件夹是专门放爬虫文件。如何生成爬虫文件？scrapy genspider spider 网址 生成的spider如下图所示: 图中出现的是最基本的四个属性与方法。其中start_urls会将列表中的网址依次传给start_requests.其中比较重要的是生成的request请求和返回的response有哪些属性与方法？ request有这些属性与方法，headers,body,cookies,meta这些 其中meta又有哪些值？ 这篇文章还不错 response的属性与方法 那如何获取到网页源码呢？可以这样写response.body.decode(response.encoding) 还有总不可能一个爬虫文件只写一个parse函数把，因此如何跳转到其他函数，这里就要用scrapy.Request中的回调函数callback,当scrapy.Request完成了请求生成了response，就会将response作为回调函数的参数传递给回调函数。那有哪些方式可以传递参数给回调函数？可以通过request的meta来传递参数。 然后是downloadermiddleware spidermiddleware和itme pipeline 下面就是个人对scrapy理解的图 其中 1是pipeline item 2是spider middleware 是downloader middlerware. 由于2基本不会怎么用到这里就来讲item,spiders，item pipeline downloadermiddlerware. item item，创建的item需要继承scrapy.Item类。，其中的对象需要定义类型为scrapy.Field的字段。 比如这个例子 class QuoteItem(scrapy.Item){ text=scrapy.Field() 从这里我们可以看出Item是类，这就可以用isinstance来判断类型。 spider 普通的scrapy是继承basic。即scrapy.Spider。 这里先放出一个有基本属性的爬虫来，再来讲爬虫的基本属性 # -*- coding: utf-8 -*- import scrapy import json from scrapy.spiders import Request from scrapyproject.items import ScrapyprojectItem class ZhaopinSpider(scrapy.Spider): name = 'zhaopin' allowed_domains = ['zhaopin.com'] /*start_urls=[‘https://fe-api.zhaopin.com/c/i/sou?pageSize=10’]*/ def start_requests(self): for i in range(1,10): url = 'https://fe-api.zhaopin.com/c/i/sou?pageSize=%d'%(i*10) yield Request(url=url) def parse(self, response): pass name是爬虫的名字。allowed_domains是允许爬取的域名，是可选配置，如果去掉，就能够爬全网。 custom_settings是一个字典，可以覆盖原本的项目全局的配置 start_requests和start_url这两个里面智能出现一个。道理是一样的。start_url里面的url会一个一个的发送到start_requests中。 parse()这个是当Response没有回调函数，该方法默认会被覅用，对了scrapy中自己写的处理爬虫数据的方法只有三种结果。Request,item,None。 closed这个是当Spider关闭是，该方法会被自动调用 这里来讲一下yield出去的Request 这里就是官方的Request对象。我们zhe李需要运用的是url,与callback. response，首先来讲一下response与Selector。这里需要理解的是xpath与re在response上的体现。如何使用xpath只要response.xpath(“//a”)就能获取到xpath对象那么如何获取到节点信息需要用到的是extract_first()与extract()。如response.xpath(“//a”).extract(),response.xpath(“//a”).前者只有第一个元素，后者获取到的是一个可迭代列表。re正则匹配又如何使用？只要在xpath后使用即可。response.xpath(“XXX”).re(xx)(其中后面的xx是正则表达式其实正则匹配也是有两个re_first()与re()) pipeline item。 自定义Item Pipeline。其实很简单只要定义一个类，这个类是需要有几个方法。分别是open_spider(spider),close_spider(spider)，from_crawler(spider,crawler)，还有一个必须的方法就是process_item(item,spider).其中open_spider(spider),是再spider开始运行时执行的，close_spider(spider)是再spider停止运行后，进行的。这两个方法有什么好处。比如说你要将数据存到数据库中，总不能每次调用一下这个Item Pipeline就执行这个连接和断开数据库的工作吧。所以可以将连接数据库open_spider(spider)，关闭数据库放在close_spider(spider)。然后处理数据就由process_item(item,spider)来执行。对了，有一个爬虫项目肯定不可能只有一个pipeline item 那么我们加入我们有两个item ,现在分别对这两个item，分别写了两个Item pipeline。那么我们应该怎么区别哪个item pipeline处理哪个item。这里就用到了前面的isinstance。只要用isinstance(object,class)根据返回的True或者False.就能知道是不是用这个Pipeline来处理这个Item. Downloader middleware 这个我们也可以重新只要有三个方法就行了，分别是process_request(request,spider)、process_response(request,response,spider)(应该会有人好奇为什么response，还需要request。这是因为当response不符合需要的要求时候，我们可以选择将request生成新的Request，返回队列和process_exception(request,exception,spider) 上面三种方法对放回不同的值会有什么处理方式： process_request(request,spider) 当返回是None时，Scrapy会继续处理该Request，去执行其他Downloader Middleware的process_request()，一直当Downloader 返回执行得到的Response才借宿。这个过程中也就是修改Request的过程，可以修改Request的代理，cookie或者是代理。那么如何修改这些呢？修改头部，request.headers[“User-Agent’]=xx(user-agent)。修改cookie，request.cookies=xx(cookies)。修改代理request.meta[“proxy”]=xx(proxy)。 当返回的是response的时候，更低优先级的Downloader Middlerware的process_request与process_exception就不会被调用，被调用的只会是更低级的process_response(request,response,spider)会被调用。（刚好与前面的返回None对应） 当返回是Request对象时候，会将该request重新生成Request放入带爬取队列中。 process_response(request,response,spider) 当返回的是response的时候，更低级别的process_response会被继续被调用。 当返回是Request对象时候，会将该request重新生成Request放入带爬取队列中。 process_response(request,exception,spider) 这个函数方法触发得到条件是当process_request或者proccess_response出现Ignorerequest异常抛出就会依次进行process_response(request,exception,spider)。当返回值是None时候，更低级别的process_response(request,exception,spider)就会被执行，直到所有的process_response(request,exception,spider)被调度完毕。 当返回的是response的时候，更低优先级的Downloader Middlerware process_exception就不会被调用，被调用的只会是更低级的process_response(request,response,spider)会被调用。（刚好与前面的返回None对应） 当返回是Request对象时候，会将该request重新生成Request放入带爬取队列中. 归纳这三个方法就是当返回是None就会执行更低级别的本命方法，当返回值是response的时候就会执行更低级别的process_response。当返回值是request，就会将request生成Request，返回爬取队列。 Copyright © dsx2016.com 2019 all right reserved，powered by Gitbook该文章修订时间： 2020-11-18 14:27:23 "},"Scrapy/Scrapy部署.html":{"url":"Scrapy/Scrapy部署.html","title":"Scrapy部署","keywords":"","body":"爬虫中一个高效率框架就是scrapy,爬取效率特别快，但是一台机子不管怎么搞都是快不上去的，因此就想到了scrapy当然可以做分布式爬虫，分布式爬虫只要完成节点间的通讯就可以了，但是scrapy有一样东西特别麻烦那就是,就是部署，比如100台机子，总不能每台都开sftp、ssh吧。这样子搞100次那不得很麻烦，而且假如要修改那不是很奔溃。 因此我们应该要解决两个问题，1、如何快速的布置项目.2、如何修改一个母代码，子代码也会修改。读完这篇文章，你能解决第一个问题。解决第一个问题的方法就是Gepray与spiderkeeper。前者是用Django，后者使用flask框架。这两个部署框架各有千秋。在介绍这两个框架之前我们还要说一下实现这两个框架的基础 scrapyd 这个是部署scrapy到服务器上的部署工具 这个是运行在你要部署的服务器上。(可以这样理解 d+scrapy。服务器得到scrapy.) scrapyd（GITHUB地址）。只要我们在服务器上运行scrapyd，本地再通过命令行就能够部署scrapy到服务器上。scrapyd通过了一系列的HTTP接口，使得能够在本地对布置在服务器上的scrapy实现各种操作。 下面来说一下（服务器ip为47.100.202.235） 如何使用crapyd 安装 ：ip3 install scrapyd 做一个配置文件 mkdir /etc/scrapyd/ vi /etc/scrapyd/scrapyd.confi 配置内容在这里：https://scrapyd.readthedocs.io/en/stable/config.html#example-configuration-file api接口 aemonstatues.json 这个接口用来查看scrapy当前的服务和任务状态。我们可以用curl来请求这个接口 curl http://47.100.202.235:6800/daemonstatus.json 返回结果 {\"pending\": 0, \"finished\": 0, \"node_name\": \"i-uf68xge6r3ivl\", \"running\": 0, \"status\": \"ok\"} 其中pending表示等待被调度的scrapyd任务，status表示当前的运行状态，finished表示已经完成的scrapy任务，running表示正在运行的scrapy任务 addversion.json 这个接口是用来部署本地egg(scrapyd只接受egg文件，如何将scrapy项目打包成egg文件,下面会讲) curl http://47.100.202.235:6800/addversion.json -F project=project_name =F version=xx -F egg=@项目打包成egg的名字.egg schedule.json 用来实现任务的调度，比如执行某个爬虫scrapy curl http://47.100.202.235:6800/schedule.json -d project=project_name -d spider=spider_name 返回 {“status”:”ok”,”jobid”:”sqgwqegeet1yo81282t33r”} 其中status表示scrapy项目的启动情况。jobid表示当前正在运行的爬取任务代号 cancel.json 表示取消摸个爬虫任务 curl http://47.100.202.235:6800/cancel.json -d project=project_name -d job= sqgwqegeet1yo81282t33r 需要传入两个参数 peoject，即项目名称。job表示任务代号。 listprojects.json curl http://47.100.202.235:6800/daemonstatus.json返回正在运行的项目 listspiders.json curl http://47.100.202.235:6800/listspiders.json 返回正在运行的爬虫项目的版本号 listjobs.json curl http://47.100.202.235:6800/listjobs.json 返回正在运行,运行结束，等待运行的爬虫项目的id scrapyd API 是对scrapyd进行封装，用python去实现，这个以后看。 scrapyd-client 这个比scrapyd多了一个功能，后者是将egg文件部署到服务器上， 前者能 将scrapy文件打包成egg文件 打包好后部署到服务器上如何使用scrapyd-client. 因为要打包的是文件夹，所以肯定是进入文件夹打包。 进入到文件夹 修改scrapy.cfg中的[deloy]中的url。 修改成http://被部署的服务器的ip:6800(比如url=http://47.100.202.235:6800) 再scrapy项目根目录输入scrapyd-cdeloy如果有多台主机的话，修改scrapy.cfg如下所示： [deloy:vm1] url = http://xx.xx.xx.1:6800 project=project_name [deloy:vm2] url = http://xx.xx.xx.2:6800 project=project_name 最后输入命令 scrapy-deloy vm1 scrapy-deloy vm2 那scrapyd如何对接docker? 讲完了以上之后，来讲下Gerapy与spiderkeeper Gerapy Gerapy与spiderkeeper就是，通过scrapyd-client来打包成egg文件。再用scrapyd的http端口来实现一些功能。 Gerapy(Github地址：https://github.com/Gerapy/Gerapy)，相对于spiderkeeper不足之处就是不能进行任务的定时运行。 如何使用Gerapy部署scrapy项目： 多台服务器 1台服务器运行gerapy,其他服务器运行scrapyd. gerapy命令： $ gerapy Usage: gerapy init [--folder=] gerapy migrate gerapy createsuperuser gerapy runserver [] gerapy makemigrations ` 如何创建gerapy 初始化： gerapy init (如果想要在哪个文件夹中创建的话，就加上这个文件夹，否则会在同层目录创建gerapy文件夹) 进入初始化的文件夹进行，数据库初始化 gerapy migrate 运行gerapy gerapy runserver ip:port 然后就运行成功了。接下来来讲下gerapy图形化操作 分别来讲下1、2、3三个操作界面 1、 是一个浏览界面没有什么可以操作的。 2、 从左到右讲。状态表示和该节点（主机）的通讯情况，就是能不能将项目布置到这个节点上。一般只要这个节点运行了scrapyd和防火墙开放了端口号。就可以访问这个节点。这个节点就会显示连接，2、表示节点的配置信息，如下图所示：其中用户名和密码是如果用了ngix才需要填上去的，一般都不用填 3表示该节点上爬虫的运行情况，如下图所示： 然后看第三部分 其中编辑比较有趣就是编辑项目的源码，每修改一次就会自动发送一个post请求。点击部署后会显示需要打包的信息，只要按钮就可以了。这个最方便的就是一次可以部署再多个节点，点击部署后如下图所示 还有如何把项目放到Project里面。很简单，之前我们gerapy init的时候再gerapy文件夹中回生成project这个文件。只要我们把scrapy拖到这个文件夹中即可。 spiderkeeper.（github地址 https://github.com/DormyMo/SpiderKeeper）相对于gerapy来说这个的不足之处比较多。1，节点的布置比较麻烦，只能在每次启动spiderkeeper的时候，才能加节点。如果在运行中需要增加节点的化就只能重启的时候增加server列表里面的ip.2、不能修改源码，这个比较不好，就是每次修改源码的时候，需要自己打开idle.但是这个不足之处，也可以说被弥补了一部分，因为gerapyd必须要打开sftp或者是ssh把文件传到中心节点上才行。而这个spiderkeeper只要在浏览器上就可以进行这个操作。） spiderkeeper.命令： spiderkeeper [options] Options: -h, --help show this help message and exit --host=HOST host, default:0.0.0.0 --port=PORT port, default:5000 --username=USERNAME basic auth username ,default: admin --password=PASSWORD basic auth password ,default: admin --type=SERVER_TYPE access spider server type, default: scrapyd --server=SERVERS servers, default: ['http://localhost:6800'] --database-url=DATABASE_URL SpiderKeeper metadata database default: sqlite:////home/souche/SpiderKeeper.db --no-auth disable basic auth -v, --verbose log level ` example: spiderkeeper --server=http://localhost:6800 下载 pip3 install spiderkeeper 启动 spiderkeeper –-host = xx.xx.xx.xx –port=xx(中心节点的ip和port,一般是直接在服务器上运行 –-host =0.0.0.0. –-port = 8000) –-username= xx –-password = xx –-server =http://其他节点的ip:其他节点运行scrapyd的端口 如： 现在来介绍图形化界面： 下图是上传egg文件，那如何生成egg文件？https://github.com/scrapy/scrapyd-client 如：在想要得到scrapy项目的egg的根目录下 python C:\\Users\\asus\\Anaconda3\\Scripts\\scrapyd-deploy --build-egg output.egg（想要得到egg的名字） Copyright © dsx2016.com 2019 all right reserved，powered by Gitbook该文章修订时间： 2020-11-18 14:20:56 "}}